---
theme: academic
transition: slide-left
title: "LoRA: Low-Rank Adaptation of Large Language Models"
---

# [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

## Presented by Nima Shoghi

---

# Background: Large Language Models

---

# Background: Fine-Tuning

---

# Problem: Fine-Tuning is Expensive



---

# Existing Solutions

- Freezing the backbone
<!-- - [Adapter Layers](http://arxiv.org/abs/1902.00751) -->
- [Prefix Tuning](https://arxiv.org/abs/2101.00190)


---

# [Adapter Layers: Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/1902.00751)

![Adapter Layer](/adapter.png)

---

# Low Intrinsic Dimensionality of Pre-Trained Models

## [Measuring the Intrinsic Dimension of Objective Landscapes](https://arxiv.org/abs/1804.08838)
## [Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning](https://arxiv.org/abs/2012.13255)

---
